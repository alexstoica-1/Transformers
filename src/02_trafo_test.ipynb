{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook continues from `transformer_architecture.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93445b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/deep-learning/lib/python3.13/site-packages/nbformat/__init__.py:96: MissingIDFieldWarning: Cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.\n",
      "  validate(nb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN(\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n",
      "TransformerEncoder(\n",
      "  (att): TransformerAttention(\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (output_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ffn): FFN(\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (LayerNorm_att): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (LayerNorm_ffn): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "TransformerDecoder(\n",
      "  (att): TransformerAttention(\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (output_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ffn): FFN(\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (LayerNorm_att1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (LayerNorm_att2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (LayerNorm_ffn): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Bring in model definitions and dependencies from the architecture notebook\n",
    "%run ./transformer_architecture.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e4968-a06e-4d8c-bd9e-43508fa232fa",
   "metadata": {},
   "source": [
    "## Testing Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f099df40-9609-407d-b3a8-0913bd77e011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocabulary size: 30522\n",
      "Input shape: torch.Size([2, 16])\n",
      "Embedded shape after projection: torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "## testing on the embedding implemntation\n",
    "## Tokenlize model input: from batched sentences to batched sequence of code\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "# layer config \n",
    "d_model = 768\n",
    "d_embed = 1024  # Larger embedding dimension\n",
    "vocab_size=30522\n",
    "\n",
    "# loading sample data\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=True, use_multiprocessing=False)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Will truncate the sequences that are longer than the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "max_position_embeddings = 512\n",
    "model_inputs = tokenizer(sequences, truncation=True,  padding=\"longest\")\n",
    "\n",
    "# Check vocabulary size from the tokenizer\n",
    "# Happen to be the same as the default setting for distilbert -- of course!\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"Tokenizer vocabulary size: {vocab_size}\")\n",
    "\n",
    "\n",
    "input = torch.tensor(model_inputs['input_ids'])\n",
    "embedder = EmbeddingWithProjection(vocab_size=vocab_size, d_embed=d_embed, d_model=d_model)\n",
    "output = embedder(input)\n",
    "\n",
    "print(f\"Input shape: {input.shape}\")\n",
    "print(f\"Embedded shape after projection: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae577d2-6ad1-4e9f-ad9b-26c646ec7c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output Statistics:\n",
      "Mean: 0.0000\n",
      "Std: 1.0000\n",
      "Min: -2.7968\n",
      "Max: 2.8519\n",
      "\n",
      "Attention Analysis:\n",
      "Unmasked positions mean: 0.8078\n",
      "Masked positions mean: 0.8078\n",
      "\n",
      "Is the masking working? Yes\n",
      "\n",
      "All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def test_transformer_encoder():\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Test parameters\n",
    "    batch_size = 32\n",
    "    seq_length = 20\n",
    "    d_model = 512\n",
    "    d_ff = 2048\n",
    "    num_heads = 8\n",
    "    \n",
    "    # Initialize the transformer encoder\n",
    "    encoder = TransformerEncoder(\n",
    "        d_model=d_model,\n",
    "        d_ff=d_ff,\n",
    "        num_head=num_heads,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # Set to evaluation mode to disable dropout\n",
    "    encoder.eval()\n",
    "    \n",
    "    # Create input sequence - using ones instead of random values\n",
    "    # for easier interpretation of attention patterns\n",
    "    input_sequence = torch.ones(batch_size, seq_length, d_model)\n",
    "    cross_sequence = torch.ones(batch_size, seq_length, d_model)*0.5\n",
    "    \n",
    "    # Create attention mask\n",
    "    attention_mask = torch.ones(batch_size, seq_length)\n",
    "    attention_mask[:, 15:] = 0  # Mask last 5 positions\n",
    "    attention_mask =attention_mask.unsqueeze(1).unsqueeze(3)\n",
    "    \n",
    "    # Store attention patterns\n",
    "    attention_patterns = []\n",
    "    \n",
    "    # Define hook to capture attention scores\n",
    "    def attention_hook(module, input, output):\n",
    "        # We want to capture the attention scores before they're processed further\n",
    "        # This assumes your attention module returns the attention scores\n",
    "        attention_patterns.append(output)\n",
    "    \n",
    "    # Register the hook on the attention computation\n",
    "    encoder.att.register_forward_hook(attention_hook)\n",
    "    \n",
    "    # Perform forward pass\n",
    "    with torch.no_grad():\n",
    "        output = encoder(input_sequence, attention_mask)\n",
    "    \n",
    "    # Basic shape tests\n",
    "    expected_shape = (batch_size, seq_length, d_model)\n",
    "    assert output.shape == expected_shape, f\"Expected shape {expected_shape}, got {output.shape}\"\n",
    "    \n",
    "    # Print output statistics\n",
    "    print(\"\\nOutput Statistics:\")\n",
    "    print(f\"Mean: {output.mean():.4f}\")\n",
    "    print(f\"Std: {output.std():.4f}\")\n",
    "    print(f\"Min: {output.min():.4f}\")\n",
    "    print(f\"Max: {output.max():.4f}\")\n",
    "    \n",
    "    # Analyze attention patterns\n",
    "    if attention_patterns:\n",
    "        attention_output = attention_patterns[0]\n",
    "        # Look at the attention patterns for unmasked vs masked positions\n",
    "        unmasked_attention = output[:, :15, :].abs().mean()\n",
    "        masked_attention = output[:, 15:, :].abs().mean()\n",
    "        \n",
    "        print(\"\\nAttention Analysis:\")\n",
    "        print(f\"Unmasked positions mean: {unmasked_attention:.4f}\")\n",
    "        print(f\"Masked positions mean: {masked_attention:.4f}\")\n",
    "        \n",
    "        # Note: We expect masked positions to still have values due to residual connections,\n",
    "        # but their patterns should be different from unmasked positions\n",
    "        print(\"\\nIs the masking working?\", \"Yes\" if unmasked_attention != masked_attention else \"No\")\n",
    "    \n",
    "    # Check for any NaN or infinite values\n",
    "    assert torch.isfinite(output).all(), \"Output contains NaN or infinite values\"\n",
    "    \n",
    "    print(\"\\nAll tests passed successfully!\")\n",
    "    return output, attention_patterns\n",
    "\n",
    "# Run the test\n",
    "output, attention_patterns = test_transformer_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94221639-1a01-48f7-b805-9113d4347ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output Statistics:\n",
      "Mean: 0.0000\n",
      "Std: 1.0000\n",
      "Min: -4.3617\n",
      "Max: 4.5787\n",
      "\n",
      "Shape Analysis:\n",
      "Input shape: torch.Size([32, 20, 512])\n",
      "Output shape: torch.Size([32, 20, 512])\n",
      "Expected shape matches: Yes\n",
      "\n",
      "All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def test_transformer_decoder():\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Test parameters\n",
    "    batch_size = 32\n",
    "    seq_length = 20\n",
    "    encoder_seq_length = 22\n",
    "    d_model = 512\n",
    "    d_ff = 2048\n",
    "    num_heads = 8\n",
    "    \n",
    "    decoder = TransformerDecoder(\n",
    "        d_model=d_model,\n",
    "        d_ff=d_ff,\n",
    "        num_head=num_heads,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Create input sequences\n",
    "    decoder_input = torch.randn(batch_size, seq_length, d_model)\n",
    "    encoder_output = torch.randn(batch_size, encoder_seq_length, d_model)\n",
    "    \n",
    "    # Create padding mask for encoder outputs\n",
    "    padding_mask = torch.ones(batch_size, seq_length, encoder_seq_length)\n",
    "    padding_mask[:, :, 18:] = 0  # Mask last 4 positions of encoder output\n",
    "    padding_mask = padding_mask.unsqueeze(1)  # Add head dimension\n",
    "    \n",
    "    # Store attention scores\n",
    "    attention_scores = []\n",
    "    \n",
    "    # Define hook to capture attention scores before softmax\n",
    "    def attention_hook(module, input, output):\n",
    "        if not attention_scores:  # Only store first layer's patterns\n",
    "            # Assuming attention scores are computed before this hook\n",
    "            attention_scores.append(module.att_matrix.detach())  # You might need to modify this based on your attention implementation\n",
    "    \n",
    "    # Register hook on the attention layer\n",
    "    decoder.att.register_forward_hook(attention_hook)\n",
    "    \n",
    "    # Perform forward pass\n",
    "    with torch.no_grad():\n",
    "        output = decoder(decoder_input, encoder_output, padding_mask)\n",
    "    \n",
    "    # Basic shape tests\n",
    "    expected_shape = (batch_size, seq_length, d_model)\n",
    "    assert output.shape == expected_shape, f\"Expected shape {expected_shape}, got {output.shape}\"\n",
    "    \n",
    "    # Print output statistics\n",
    "    print(\"\\nOutput Statistics:\")\n",
    "    print(f\"Mean: {output.mean():.4f}\")\n",
    "    print(f\"Std: {output.std():.4f}\")\n",
    "    print(f\"Min: {output.min():.4f}\")\n",
    "    print(f\"Max: {output.max():.4f}\")\n",
    "    \n",
    "    # Test shape preservation\n",
    "    print(\"\\nShape Analysis:\")\n",
    "    print(f\"Input shape: {decoder_input.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Expected shape matches: {'Yes' if decoder_input.shape == output.shape else 'No'}\")\n",
    "    \n",
    "    # Check for any NaN or infinite values\n",
    "    assert torch.isfinite(output).all(), \"Output contains NaN or infinite values\"\n",
    "    \n",
    "    print(\"\\nAll tests passed successfully!\")\n",
    "    return output, attention_scores\n",
    "\n",
    "# Run the test\n",
    "output, attention_scores = test_transformer_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab402c1-48a7-48b1-9202-3d2a43e186c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoder Layer 0 shape: torch.Size([8, 10, 512])\n",
      "\n",
      "Encoder Layer 1 shape: torch.Size([8, 10, 512])\n",
      "\n",
      "Encoder Layer 2 shape: torch.Size([8, 10, 512])\n",
      "\n",
      "Encoder Layer 3 shape: torch.Size([8, 10, 512])\n",
      "\n",
      "Encoder Layer 4 shape: torch.Size([8, 10, 512])\n",
      "\n",
      "Encoder Layer 5 shape: torch.Size([8, 10, 512])\n",
      "Decoder Layer 0 shape: torch.Size([8, 10, 512])\n",
      "Decoder Layer 1 shape: torch.Size([8, 10, 512])\n",
      "Decoder Layer 2 shape: torch.Size([8, 10, 512])\n",
      "Decoder Layer 3 shape: torch.Size([8, 10, 512])\n",
      "Decoder Layer 4 shape: torch.Size([8, 10, 512])\n",
      "Decoder Layer 5 shape: torch.Size([8, 10, 512])\n",
      "\n",
      "Final Output Statistics:\n",
      "Mean: 0.0000\n",
      "Std: 1.0000\n",
      "Min: -3.7172\n",
      "Max: 4.1310\n",
      "\n",
      "Shape Preservation Check:\n",
      "Input shapes - Encoder: torch.Size([8, 10, 512]), Decoder: torch.Size([8, 10, 512])\n",
      "Output shape: torch.Size([8, 10, 512])\n",
      "\n",
      "Mean absolute difference between input and output: 0.9379\n",
      "Transformation occurred: Yes\n",
      "\n",
      "Total number of parameters: 37,834,752\n",
      "\n",
      "All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def test_transformer_encoder_decoder_stack():\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Test parameters\n",
    "    batch_size = 8\n",
    "    seq_length = 10\n",
    "    d_model = 512\n",
    "    d_ff = 2048\n",
    "    num_heads = 8\n",
    "    num_layers = 6\n",
    "    \n",
    "    # Initialize the transformer encoder-decoder stack\n",
    "    transformer = TransformerEncoderDecoder(\n",
    "        num_layer=num_layers,\n",
    "        d_model=d_model,\n",
    "        d_ff=d_ff,\n",
    "        num_head=num_heads,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # Set to evaluation mode to disable dropout\n",
    "    transformer.eval()\n",
    "    \n",
    "    # Create input sequences\n",
    "    encoder_input = torch.randn(batch_size, seq_length, d_model)\n",
    "    decoder_input = torch.randn(batch_size, seq_length, d_model)\n",
    "    \n",
    "    # Create padding mask\n",
    "    padding_mask = torch.ones(batch_size, seq_length)\n",
    "    padding_mask[:, -2:] = 0  # Mask last 2 positions\n",
    "    padding_mask = padding_mask.unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, seq_len]\n",
    "    \n",
    "    # Store intermediate outputs\n",
    "    intermediate_outputs = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        intermediate_outputs.append(output.detach())\n",
    "    \n",
    "    # Register hooks to capture outputs from each encoder and decoder layer\n",
    "    for i, (encoder, decoder) in enumerate(zip(transformer.encoder_stack, transformer.decoder_stack)):\n",
    "        encoder.register_forward_hook(lambda m, i, o, layer=i: print(f\"\\nEncoder Layer {layer} shape:\", o.shape))\n",
    "        decoder.register_forward_hook(lambda m, i, o, layer=i: print(f\"Decoder Layer {layer} shape:\", o.shape))\n",
    "    \n",
    "    # Perform forward pass\n",
    "    with torch.no_grad():\n",
    "        output = transformer(encoder_input, decoder_input, padding_mask)\n",
    "    \n",
    "    # Basic shape tests\n",
    "    expected_shape = (batch_size, seq_length, d_model)\n",
    "    assert output.shape == expected_shape, f\"Expected shape {expected_shape}, got {output.shape}\"\n",
    "    \n",
    "    # Print output statistics\n",
    "    print(\"\\nFinal Output Statistics:\")\n",
    "    print(f\"Mean: {output.mean():.4f}\")\n",
    "    print(f\"Std: {output.std():.4f}\")\n",
    "    print(f\"Min: {output.min():.4f}\")\n",
    "    print(f\"Max: {output.max():.4f}\")\n",
    "    \n",
    "    # Verify shape preservation through layers\n",
    "    print(\"\\nShape Preservation Check:\")\n",
    "    print(f\"Input shapes - Encoder: {encoder_input.shape}, Decoder: {decoder_input.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "    # Check for any NaN or infinite values\n",
    "    assert torch.isfinite(output).all(), \"Output contains NaN or infinite values\"\n",
    "    \n",
    "    # Verify that output is different from input (transformation happened)\n",
    "    input_output_diff = (output - decoder_input).abs().mean()\n",
    "    print(f\"\\nMean absolute difference between input and output: {input_output_diff:.4f}\")\n",
    "    print(\"Transformation occurred:\", \"Yes\" if input_output_diff > 1e-3 else \"No\")\n",
    "    \n",
    "    # Check if model parameters were used\n",
    "    total_params = sum(p.numel() for p in transformer.parameters())\n",
    "    print(f\"\\nTotal number of parameters: {total_params:,}\")\n",
    "    \n",
    "    print(\"\\nAll tests passed successfully!\")\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "output = test_transformer_encoder_decoder_stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe316a8-d66f-417f-93e9-4450e4378e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Shapes:\n",
      "Source tokens: torch.Size([2, 16])\n",
      "Target tokens: torch.Size([2, 17])\n",
      "\n",
      "Output Analysis:\n",
      "Output shape: torch.Size([2, 17, 30522])\n",
      "\n",
      "Probability Distribution Check:\n",
      "Sum to 1: True\n",
      "Max probability: 0.0005\n",
      "Min probability: 0.0000\n",
      "\n",
      "Sample Predictions:\n",
      "Original target:\n",
      "J'ai attendu un cours HuggingFace toute ma vie.\n",
      "\n",
      "Model output (decoded):\n",
      "##aco bearer barriedate gate spoil lowlands tam navigation growls 1971 painfully demand negativelyzam [unused158] lowlands\n",
      "\n",
      "Training Check:\n",
      "Loss value: 10.7329\n",
      "Has gradients: True\n"
     ]
    }
   ],
   "source": [
    "def test_complete_transformer():\n",
    "    # Configuration\n",
    "    d_model = 768\n",
    "    d_embed = 1024\n",
    "    d_ff = 2048\n",
    "    num_heads = 8\n",
    "    num_layers = 6\n",
    "    max_position_embeddings = 512\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\", \n",
    "                                            use_fast=True, \n",
    "                                            use_multiprocessing=False)\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    \n",
    "    # Create sample source and target sequences\n",
    "    src_sequences = [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"So have I!\"\n",
    "    ]\n",
    "    # Pretend these are translations\n",
    "    tgt_sequences = [\n",
    "        \"J'ai attendu un cours HuggingFace toute ma vie.\",\n",
    "        \"Moi aussi!\"\n",
    "    ]\n",
    "    \n",
    "    # Tokenize source and target sequences\n",
    "    src_inputs = tokenizer(src_sequences, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "    tgt_inputs = tokenizer(tgt_sequences, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "    \n",
    "    # Create transformer model\n",
    "    transformer = Transformer(\n",
    "        num_layer=num_layers,\n",
    "        d_model=d_model,\n",
    "        d_embed=d_embed,\n",
    "        d_ff=d_ff,\n",
    "        num_head=num_heads,\n",
    "        src_vocab_size=vocab_size,\n",
    "        tgt_vocab_size=vocab_size,\n",
    "        max_position_embeddings=max_position_embeddings\n",
    "    )\n",
    "    \n",
    "    # Set to eval mode\n",
    "    transformer.eval()\n",
    "    \n",
    "    # Create padding mask from attention mask\n",
    "    padding_mask = src_inputs['attention_mask'].unsqueeze(1).unsqueeze(2)\n",
    "    \n",
    "    print(\"\\nInput Shapes:\")\n",
    "    print(f\"Source tokens: {src_inputs['input_ids'].shape}\")\n",
    "    print(f\"Target tokens: {tgt_inputs['input_ids'].shape}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = transformer(\n",
    "            src_tokens=src_inputs['input_ids'],\n",
    "            tgt_tokens=tgt_inputs['input_ids'],\n",
    "            padding_mask=padding_mask\n",
    "        )\n",
    "    \n",
    "    print(\"\\nOutput Analysis:\")\n",
    "    print(f\"Output shape: {output.shape}\")  # Should be [batch_size, tgt_len, vocab_size]\n",
    "    \n",
    "    # Verify output is proper probability distribution\n",
    "    print(\"\\nProbability Distribution Check:\")\n",
    "    print(f\"Sum to 1: {torch.allclose(output.exp().sum(dim=-1), torch.ones_like(output.exp().sum(dim=-1)))}\")\n",
    "    print(f\"Max probability: {output.exp().max().item():.4f}\")\n",
    "    print(f\"Min probability: {output.exp().min().item():.4f}\")\n",
    "    \n",
    "    # Check if we can get predictions\n",
    "    predictions = output.argmax(dim=-1)\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(\"Original target:\")\n",
    "    print(tgt_sequences[0])\n",
    "    print(\"\\nModel output (decoded):\")\n",
    "    print(tokenizer.decode(predictions[0]))\n",
    "    \n",
    "    # Test backward pass\n",
    "    transformer.train()\n",
    "    output = transformer(\n",
    "        src_tokens=src_inputs['input_ids'],\n",
    "        tgt_tokens=tgt_inputs['input_ids'],\n",
    "        padding_mask=padding_mask\n",
    "    )\n",
    "    \n",
    "    # Calculate loss (cross entropy)\n",
    "    loss = F.nll_loss(\n",
    "        output.view(-1, vocab_size),\n",
    "        tgt_inputs['input_ids'].view(-1)\n",
    "    )\n",
    "    \n",
    "    # Test backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Verify gradients\n",
    "    has_gradients = all(p.grad is not None for p in transformer.parameters())\n",
    "    print(\"\\nTraining Check:\")\n",
    "    print(f\"Loss value: {loss.item():.4f}\")\n",
    "    print(f\"Has gradients: {has_gradients}\")\n",
    "    \n",
    "    return output, predictions\n",
    "\n",
    "# Run test\n",
    "output, predictions = test_complete_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Continue with visual exploration -> `transformer_visualization.ipynb`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

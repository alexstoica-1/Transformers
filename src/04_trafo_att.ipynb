{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook focuses on attention diagnostics and follows `transformer_visualization.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf1b24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/deep-learning/lib/python3.13/site-packages/nbformat/__init__.py:96: MissingIDFieldWarning: Cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.\n",
      "  validate(nb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN(\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n",
      "TransformerEncoder(\n",
      "  (att): TransformerAttention(\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (output_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ffn): FFN(\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (LayerNorm_att): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (LayerNorm_ffn): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "TransformerDecoder(\n",
      "  (att): TransformerAttention(\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (output_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ffn): FFN(\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (LayerNorm_att1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (LayerNorm_att2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (LayerNorm_ffn): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Bring in model definitions and dependencies from the architecture notebook\n",
    "%run ./transformer_architecture.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fca5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def _set_axis_labels(ax, tokens, axis):\n",
    "    if tokens is None:\n",
    "        return\n",
    "    positions = range(len(tokens))\n",
    "    if axis == \"x\":\n",
    "        ax.set_xticks(list(positions))\n",
    "        ax.set_xticklabels(tokens, rotation=45, ha=\"right\")\n",
    "    else:\n",
    "        ax.set_yticks(list(positions))\n",
    "        ax.set_yticklabels(tokens)\n",
    "\n",
    "def plot_attention_heatmaps(att_weights, head_indices=None, query_tokens=None, key_tokens=None, title=None, cmap=\"magma\", max_heads=4, batch_index=0):\n",
    "    \"\"\"Visualize per-head attention weights as heatmaps.\"\"\"\n",
    "    if not isinstance(att_weights, torch.Tensor):\n",
    "        att_weights = torch.as_tensor(att_weights)\n",
    "    att = att_weights.detach().cpu()\n",
    "    if att.dim() != 4:\n",
    "        raise ValueError(\"att_weights must have shape [batch, heads, query_len, key_len]\")\n",
    "    num_heads = att.size(1)\n",
    "    if head_indices is None:\n",
    "        head_indices = list(range(min(max_heads, num_heads)))\n",
    "    if not head_indices:\n",
    "        raise ValueError(\"Provide at least one head index to visualize.\")\n",
    "    fig, axes = plt.subplots(1, len(head_indices), figsize=(4.5 * len(head_indices), 4), squeeze=False, layout='constrained')\n",
    "    axes = axes[0]\n",
    "    subset = att[batch_index, head_indices]\n",
    "    vmin = float(subset.min())\n",
    "    vmax = float(subset.max())\n",
    "    if np.isclose(vmax - vmin, 0.0):\n",
    "        vmax = vmin + 1e-6\n",
    "    for ax, head_idx in zip(axes, head_indices):\n",
    "        head_map = att[batch_index, head_idx]\n",
    "        if key_tokens is not None and len(key_tokens) != head_map.size(-1):\n",
    "            raise ValueError(\"Number of key_tokens must match key length.\")\n",
    "        if query_tokens is not None and len(query_tokens) != head_map.size(-2):\n",
    "            raise ValueError(\"Number of query_tokens must match query length.\")\n",
    "        im = ax.imshow(head_map, cmap=cmap, vmin=vmin, vmax=vmax, aspect=\"auto\")\n",
    "        ax.set_title(f\"Head {head_idx}\")\n",
    "        ax.set_xlabel(\"Key Positions\")\n",
    "        ax.set_ylabel(\"Query Positions\")\n",
    "        _set_axis_labels(ax, key_tokens, \"x\")\n",
    "        _set_axis_labels(ax, query_tokens, \"y\")\n",
    "    fig.suptitle(title or \"Attention Heatmaps\", fontsize=13)\n",
    "    cbar = fig.colorbar(im, ax=axes, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(\"Attention Weight\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_mean_attention(att_weights, query_tokens=None, key_tokens=None, title=None, cmap=\"magma\", batch_index=0):\n",
    "    \"\"\"Plot the mean attention pattern across all heads.\"\"\"\n",
    "    if not isinstance(att_weights, torch.Tensor):\n",
    "        att_weights = torch.as_tensor(att_weights)\n",
    "    att = att_weights.detach().cpu()\n",
    "    if att.dim() != 4:\n",
    "        raise ValueError(\"att_weights must have shape [batch, heads, query_len, key_len]\")\n",
    "    mean_map = att.mean(dim=1)[batch_index]\n",
    "    if key_tokens is not None and len(key_tokens) != mean_map.size(-1):\n",
    "        raise ValueError(\"Number of key_tokens must match key length.\")\n",
    "    if query_tokens is not None and len(query_tokens) != mean_map.size(-2):\n",
    "        raise ValueError(\"Number of query_tokens must match query length.\")\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4), layout='constrained')\n",
    "    im = ax.imshow(mean_map, cmap=cmap, aspect=\"auto\")\n",
    "    ax.set_title(title or \"Average Attention\")\n",
    "    ax.set_xlabel(\"Key Positions\")\n",
    "    ax.set_ylabel(\"Query Positions\")\n",
    "    _set_axis_labels(ax, key_tokens, \"x\")\n",
    "    _set_axis_labels(ax, query_tokens, \"y\")\n",
    "    cbar = fig.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(\"Attention Weight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4aa076-babd-4677-8e7b-5661debf5d30",
   "metadata": {},
   "source": [
    "### Attention Pattern Visualization\n",
    "Shows:\n",
    "- Self-attention patterns (causal masking)\n",
    "- Cross-attention patterns\n",
    "- Effect of padding masks\n",
    "- How attention weights distribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e9d8c0-7a6a-4c50-85ef-0e805f785f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention Matrix Shape: torch.Size([2, 8, 5, 5])\n",
      "\n",
      "Attention Pattern (first head):\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4465, 0.5535, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3403, 0.3496, 0.3101, 0.0000, 0.0000],\n",
      "        [0.1965, 0.3485, 0.1174, 0.3377, 0.0000],\n",
      "        [0.1563, 0.1571, 0.1859, 0.1948, 0.3059]])\n",
      "\n",
      "Future Token Analysis:\n",
      "Mean attention to future tokens: 0.00000000\n",
      "Max attention to future tokens: 0.00000000\n",
      "Causal masking working: Yes\n",
      "\n",
      "Present/Past Token Analysis:\n",
      "Mean attention to present/past tokens: 0.3333\n",
      "Has non-zero attention patterns: Yes\n",
      "\n",
      "Attention Sum Analysis:\n",
      "Mean attention sum (should be 1): 1.0000\n",
      "Max deviation from 1: 0.00000012\n"
     ]
    }
   ],
   "source": [
    "def test_decoder_causal_masking():\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Test parameters\n",
    "    batch_size = 2\n",
    "    seq_length = 5\n",
    "    d_model = 512\n",
    "    d_ff = 2048\n",
    "    num_heads = 8\n",
    "    \n",
    "    decoder = TransformerDecoder(\n",
    "        d_model=d_model,\n",
    "        d_ff=d_ff,\n",
    "        num_head=num_heads,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    decoder.eval()\n",
    "    \n",
    "    decoder_input = torch.randn(batch_size, seq_length, d_model)\n",
    "    encoder_output = torch.randn(batch_size, seq_length, d_model)\n",
    "    \n",
    "    attention_scores = []\n",
    "    \n",
    "    def attention_hook(module, input, output):\n",
    "        if not attention_scores:\n",
    "            # Apply softmax to get actual attention probabilities\n",
    "            scores = F.softmax(module.att_matrix, dim=-1)\n",
    "            attention_scores.append(scores.detach())\n",
    "    \n",
    "    decoder.att.register_forward_hook(attention_hook)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = decoder(decoder_input, encoder_output)\n",
    "    \n",
    "    att_weights = attention_scores[0]\n",
    "    \n",
    "    print(\"\\nAttention Matrix Shape:\", att_weights.shape)\n",
    "    \n",
    "    # Print attention pattern for first head of first batch\n",
    "    print(\"\\nAttention Pattern (first head):\")\n",
    "    print(att_weights[0, 0].round(decimals=4))\n",
    "    \n",
    "    # Check future tokens (should be 0)\n",
    "    future_attention = att_weights[:, :, torch.triu_indices(seq_length, seq_length, offset=1)[0], \n",
    "                                        torch.triu_indices(seq_length, seq_length, offset=1)[1]]\n",
    "    \n",
    "    print(\"\\nFuture Token Analysis:\")\n",
    "    print(f\"Mean attention to future tokens: {future_attention.mean():.8f}\")\n",
    "    print(f\"Max attention to future tokens: {future_attention.max():.8f}\")\n",
    "    print(\"Causal masking working:\", \"Yes\" if future_attention.mean() < 1e-7 else \"No\")\n",
    "    \n",
    "    # Check present/past tokens\n",
    "    present_past = att_weights[:, :, torch.tril_indices(seq_length, seq_length)[0],\n",
    "                                    torch.tril_indices(seq_length, seq_length)[1]]\n",
    "    \n",
    "    print(\"\\nPresent/Past Token Analysis:\")\n",
    "    print(f\"Mean attention to present/past tokens: {present_past.mean():.4f}\")\n",
    "    print(f\"Has non-zero attention patterns:\", \"Yes\" if present_past.mean() > 0 else \"No\")\n",
    "    \n",
    "    # Verify each position's attention sums to 1\n",
    "    attention_sums = att_weights.sum(dim=-1)\n",
    "    print(\"\\nAttention Sum Analysis:\")\n",
    "    print(f\"Mean attention sum (should be 1): {attention_sums.mean():.4f}\")\n",
    "    print(f\"Max deviation from 1: {(attention_sums - 1).abs().max():.8f}\")\n",
    "    \n",
    "    return att_weights\n",
    "\n",
    "attention_weights = test_decoder_causal_masking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1623643-fa64-4f9e-b399-d9d0fc6c0f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Attention Matrix Shape: torch.Size([2, 8, 5, 7])\n",
      "\n",
      "Cross-Attention Pattern (first head):\n",
      "tensor([[0.1308, 0.1502, 0.1380, 0.1131, 0.1987, 0.1117, 0.1576],\n",
      "        [0.1303, 0.1041, 0.1502, 0.1756, 0.1679, 0.1589, 0.1130],\n",
      "        [0.0896, 0.2159, 0.1142, 0.1718, 0.1797, 0.0844, 0.1444],\n",
      "        [0.1250, 0.1650, 0.1607, 0.1053, 0.0868, 0.2349, 0.1223],\n",
      "        [0.1637, 0.0842, 0.2093, 0.1223, 0.1274, 0.1392, 0.1540]])\n",
      "\n",
      "Cross-Attention Analysis:\n",
      "Mean attention weight: 0.1429\n",
      "Min attention weight: 0.0389\n",
      "Max attention weight: 0.4142\n",
      "\n",
      "Attention Coverage:\n",
      "Each position's attention sums to 1: True\n",
      "Every decoder position attends to some encoder position: True\n",
      "\n",
      "Attention entropy (higher means more uniform attention): 1.8917\n"
     ]
    }
   ],
   "source": [
    "def test_decoder_cross_attention():\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Test parameters\n",
    "    batch_size = 2\n",
    "    decoder_seq_len = 5\n",
    "    encoder_seq_len = 7  # Different length to make it interesting!\n",
    "    d_model = 512\n",
    "    d_ff = 2048\n",
    "    num_heads = 8\n",
    "    \n",
    "    decoder = TransformerDecoder(\n",
    "        d_model=d_model,\n",
    "        d_ff=d_ff,\n",
    "        num_head=num_heads,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Create input sequences\n",
    "    decoder_input = torch.randn(batch_size, decoder_seq_len, d_model)\n",
    "    encoder_output = torch.randn(batch_size, encoder_seq_len, d_model)\n",
    "    \n",
    "    # Store attention scores\n",
    "    cross_attention_scores = []\n",
    "    \n",
    "    def attention_hook(module, input, output):\n",
    "        # We want the second call to att (cross-attention)\n",
    "        if len(cross_attention_scores) < 2:\n",
    "            scores = F.softmax(module.att_matrix, dim=-1)\n",
    "            cross_attention_scores.append(scores.detach())\n",
    "    \n",
    "    decoder.att.register_forward_hook(attention_hook)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = decoder(decoder_input, encoder_output)\n",
    "    \n",
    "    # Get cross-attention weights (second element in list)\n",
    "    cross_att_weights = cross_attention_scores[1]  # [batch, heads, decoder_seq_len, encoder_seq_len]\n",
    "    \n",
    "    print(\"\\nCross-Attention Matrix Shape:\", cross_att_weights.shape)\n",
    "    \n",
    "    # Print attention pattern for first head of first batch\n",
    "    print(\"\\nCross-Attention Pattern (first head):\")\n",
    "    print(cross_att_weights[0, 0].round(decimals=4))\n",
    "    \n",
    "    # Verify each decoder position attends to all encoder positions\n",
    "    attention_sums = cross_att_weights.sum(dim=-1)\n",
    "    zero_attention = (cross_att_weights == 0).all(dim=-1)\n",
    "    \n",
    "    print(\"\\nCross-Attention Analysis:\")\n",
    "    print(f\"Mean attention weight: {cross_att_weights.mean():.4f}\")\n",
    "    print(f\"Min attention weight: {cross_att_weights.min():.4f}\")\n",
    "    print(f\"Max attention weight: {cross_att_weights.max():.4f}\")\n",
    "    \n",
    "    print(\"\\nAttention Coverage:\")\n",
    "    print(f\"Each position's attention sums to 1: {torch.allclose(attention_sums, torch.ones_like(attention_sums))}\")\n",
    "    print(f\"Every decoder position attends to some encoder position: {not zero_attention.any()}\")\n",
    "    \n",
    "    # Check attention distribution\n",
    "    attention_entropy = -(cross_att_weights * torch.log(cross_att_weights + 1e-9)).sum(dim=-1).mean()\n",
    "    print(f\"\\nAttention entropy (higher means more uniform attention): {attention_entropy:.4f}\")\n",
    "    \n",
    "    return cross_att_weights\n",
    "\n",
    "# Run the test\n",
    "cross_attention_weights = test_decoder_cross_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c7a7533-75c3-4575-af0a-cdc90e8b4815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Attention Matrix Shape: torch.Size([2, 8, 5, 7])\n",
      "\n",
      "Cross-Attention Pattern (first head):\n",
      "(Last two encoder positions should have zero attention)\n",
      "tensor([[0.1791, 0.2055, 0.1888, 0.1547, 0.2719, 0.0000, 0.0000],\n",
      "        [0.1789, 0.1430, 0.2063, 0.2412, 0.2306, 0.0000, 0.0000],\n",
      "        [0.1162, 0.2800, 0.1480, 0.2228, 0.2330, 0.0000, 0.0000],\n",
      "        [0.1945, 0.2566, 0.2500, 0.1638, 0.1350, 0.0000, 0.0000],\n",
      "        [0.2316, 0.1191, 0.2961, 0.1730, 0.1802, 0.0000, 0.0000]])\n",
      "\n",
      "Masking Analysis:\n",
      "Mean attention to masked positions: 0.00000000\n",
      "Max attention to masked positions: 0.00000000\n",
      "Mean attention to unmasked positions: 0.2000\n",
      "\n",
      "Attention Coverage:\n",
      "Each position's attention sums to 1: True\n",
      "\n",
      "Unmasked Position Analysis:\n",
      "Min attention to unmasked positions: 0.0458\n",
      "Max attention to unmasked positions: 0.4875\n"
     ]
    }
   ],
   "source": [
    "def test_decoder_cross_attention_with_padding():\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Test parameters\n",
    "    batch_size = 2\n",
    "    decoder_seq_len = 5\n",
    "    encoder_seq_len = 7\n",
    "    d_model = 512\n",
    "    d_ff = 2048\n",
    "    num_heads = 8\n",
    "    \n",
    "    decoder = TransformerDecoder(\n",
    "        d_model=d_model,\n",
    "        d_ff=d_ff,\n",
    "        num_head=num_heads,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Create input sequences\n",
    "    decoder_input = torch.randn(batch_size, decoder_seq_len, d_model)\n",
    "    encoder_output = torch.randn(batch_size, encoder_seq_len, d_model)\n",
    "    \n",
    "    # Create padding mask for encoder outputs\n",
    "    # Mask out last 2 positions (as if they were padding in encoder output)\n",
    "    padding_mask = torch.ones(batch_size, decoder_seq_len, encoder_seq_len)\n",
    "    padding_mask[:, :, -2:] = float('-inf')  # Mask positions 5,6\n",
    "    padding_mask = padding_mask.unsqueeze(1)  # Add head dimension [batch, 1, decoder_seq, encoder_seq]\n",
    "    \n",
    "    cross_attention_scores = []\n",
    "    \n",
    "    def attention_hook(module, input, output):\n",
    "        if len(cross_attention_scores) < 2:\n",
    "            scores = F.softmax(module.att_matrix, dim=-1)\n",
    "            cross_attention_scores.append(scores.detach())\n",
    "    \n",
    "    decoder.att.register_forward_hook(attention_hook)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = decoder(decoder_input, encoder_output, padding_mask)\n",
    "    \n",
    "    # Get cross-attention weights (second element)\n",
    "    cross_att_weights = cross_attention_scores[1]\n",
    "    \n",
    "    print(\"\\nCross-Attention Matrix Shape:\", cross_att_weights.shape)\n",
    "    \n",
    "    print(\"\\nCross-Attention Pattern (first head):\")\n",
    "    print(\"(Last two encoder positions should have zero attention)\")\n",
    "    print(cross_att_weights[0, 0].round(decimals=4))\n",
    "    \n",
    "    # Analyze masked positions (last two columns)\n",
    "    masked_attention = cross_att_weights[:, :, :, -2:]\n",
    "    unmasked_attention = cross_att_weights[:, :, :, :-2]\n",
    "    \n",
    "    print(\"\\nMasking Analysis:\")\n",
    "    print(f\"Mean attention to masked positions: {masked_attention.mean():.8f}\")\n",
    "    print(f\"Max attention to masked positions: {masked_attention.max():.8f}\")\n",
    "    print(f\"Mean attention to unmasked positions: {unmasked_attention.mean():.4f}\")\n",
    "    \n",
    "    # Verify attention still sums to 1 (only over unmasked positions)\n",
    "    attention_sums = cross_att_weights.sum(dim=-1)\n",
    "    \n",
    "    print(\"\\nAttention Coverage:\")\n",
    "    print(f\"Each position's attention sums to 1: {torch.allclose(attention_sums, torch.ones_like(attention_sums), atol=1e-6)}\")\n",
    "    \n",
    "    # Analyze attention distribution over unmasked positions\n",
    "    print(\"\\nUnmasked Position Analysis:\")\n",
    "    print(f\"Min attention to unmasked positions: {unmasked_attention.min():.4f}\")\n",
    "    print(f\"Max attention to unmasked positions: {unmasked_attention.max():.4f}\")\n",
    "    \n",
    "    return cross_att_weights\n",
    "\n",
    "# Run the test\n",
    "cross_attention_weights = test_decoder_cross_attention_with_padding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Return to `transformer_architecture.ipynb` or `transformer_testing.ipynb` to revisit core components.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

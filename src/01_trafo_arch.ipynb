{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a0471a-205d-479e-a9c1-94c4384d616d",
   "metadata": {},
   "source": [
    "# Implementing Transformer Architecture\n",
    "\n",
    "## Paper Reference\n",
    "- [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)\n",
    "- Key sections: \n",
    "  - 3.1: Encoder and Decoder Stacks\n",
    "  - 3.2: Attention Mechanism\n",
    "  - 3.3: Position-wise Feed-Forward Networks\n",
    "  - 3.4: Embeddings and Softmax\n",
    "  - 3.5: Positional Encoding\n",
    "  - 5.4: Regularization (dropout strategy)\n",
    "\n",
    "## Implementation Strategy\n",
    "Breaking down the architecture into manageable pieces and gradually adding complexity:\n",
    "\n",
    "1. Start with foundational components:\n",
    "   - Embedding + Positional Encoding\n",
    "   - Single-head self-attention\n",
    "   \n",
    "2. Build up attention mechanism:\n",
    "   - Extend to multi-head attention\n",
    "   - Add cross-attention capability\n",
    "   - Implement attention masking\n",
    "\n",
    "3. Construct larger components:\n",
    "   - Encoder (self-attention + FFN)\n",
    "   - Decoder (masked self-attention + cross-attention + FFN)\n",
    "   \n",
    "4. Combine into final architecture:\n",
    "   - Encoder-Decoder stack\n",
    "   - Full Transformer with input/output layers\n",
    "\n",
    "\n",
    "## Testing Strategy\n",
    "- Test each component independently\n",
    "- Verify shape preservation\n",
    "- Check attention patterns\n",
    "- Confirm mask effectiveness\n",
    "- Validate gradient flow\n",
    "- Monitor numerical stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c1308f-2592-4272-91c3-40184625bbfe",
   "metadata": {},
   "source": [
    "## Code Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9311b3a3-719c-4440-862d-2be962eff271",
   "metadata": {},
   "source": [
    "### Embedding and Positional Encoding\n",
    "This implements the input embedding from Section 3.4 and positional encoding from Section 3.5 of the paper. Key points:\n",
    "- Embedding dimension can differ from model dimension (using projection)\n",
    "- Positional encoding uses sine and cosine functions\n",
    "- Scale embeddings by √d_model\n",
    "- Apply dropout to the sum of embeddings and positional encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ce6f4c3-41b2-4610-a68e-fef96c8fe761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmbeddingWithProjection(nn.Module):\n",
    "    def __init__(self, vocab_size, d_embed, d_model,  \n",
    "                 max_position_embeddings =512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_embed = d_embed\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_embed)\n",
    "        self.projection = nn.Linear(self.d_embed, self.d_model)\n",
    "        self.scaling = float(math.sqrt(self.d_model))\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(self.d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_positional_encoding(seq_length, d_model, batch_size=1):\n",
    "        # Create position indices: [seq_length, 1]\n",
    "        position = torch.arange(seq_length).unsqueeze(1).float()\n",
    "        \n",
    "        # Create dimension indices: [1, d_model//2]\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * \n",
    "            (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        # Create empty tensor: [seq_length, d_model]\n",
    "        pe = torch.zeros(seq_length, d_model)\n",
    "        \n",
    "        # Compute sin and cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension and expand: [batch_size, seq_length, d_model]\n",
    "        pe = pe.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        return pe\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert x.dtype == torch.long, f\"Input tensor must have dtype torch.long, got {x.dtype}\"\n",
    "        batch_size, seq_length = x.size() # [batch, seq_length]\n",
    "\n",
    "        # token embedding\n",
    "        token_embedding = self.embedding(x)                                                            #[2, 16, 1024]     \n",
    "        # project the scaled token embedding to the d_model space\n",
    "        token_embedding =  self.projection(token_embedding) * self.scaling                             #[2, 16, 768]\n",
    "\n",
    "        # add positional encodings to projected, \n",
    "        # scaled embeddings before applying layer norm and dropout.\n",
    "        positional_encoding = self.create_positional_encoding(seq_length, self.d_model, batch_size)    #[2, 16, 768]\n",
    "        \n",
    "        # In addition, we apply dropout to the sums of the embeddings \n",
    "        # in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\n",
    "        normalized_sum = self.layernorm(token_embedding + positional_encoding)\n",
    "        final_output = self.dropout(normalized_sum)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d449b6-c6ea-4634-b071-0839841e3044",
   "metadata": {},
   "source": [
    "### Transformer Attention\n",
    "Implements the core attention mechanism from Section 3.2.1. Formula:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^{\\mathsf{T}}}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Key points:\n",
    "- Supports both self-attention and cross-attention\n",
    "- Multi-head attention implementation per section 3.2.2\n",
    "- Handles different sequence lengths for encoder/decoder\n",
    "- Scales dot products by 1/√d_k\n",
    "- Applies attention masking before softmax\n",
    "- Applies dropout after softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e63e4cf5-2126-4480-ac12-f24d0912e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "\n",
    "class TransformerAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Scaled Dot Product Attention Module\n",
    "    Args:\n",
    "        d_model: Total dimension of the model.\n",
    "        num_head: Number of attention heads.\n",
    "        dropout: Dropout rate for attention scores.\n",
    "        bias: Whether to include bias in linear projections.\n",
    "\n",
    "    Inputs:\n",
    "        sequence: input sequence for self-attention and the query for cross-attention\n",
    "        key_value_state: input for the key, values for cross-attention\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_head, dropout=0.1, bias=True): # infer d_k, d_v, d_q from d_model\n",
    "        super().__init__()  # Missing in the original implementation\n",
    "        assert d_model % num_head == 0, \"d_model must be divisible by num_head\"\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "        self.d_head=d_model//num_head\n",
    "        self.dropout_rate = dropout  # Store dropout rate separately\n",
    "\n",
    "        # linear transformations\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.output_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Initiialize scaler\n",
    "        self.scaler = float(1.0 / math.sqrt(self.d_head)) # Store as float in initialization\n",
    "        \n",
    "\n",
    "    def forward(self, sequence, key_value_states = None, att_mask=None):\n",
    "        \"\"\"Input shape: [batch_size, seq_len, d_model=num_head * d_head]\"\"\"\n",
    "        batch_size, seq_len, model_dim = sequence.size()\n",
    "\n",
    "        # Check only critical input dimensions\n",
    "        assert model_dim == self.d_model, f\"Input dimension {model_dim} doesn't match model dimension {self.d_model}\"\n",
    "        if key_value_states is not None:\n",
    "            assert key_value_states.size(-1) == self.d_model, \\\n",
    "            f\"Cross attention key/value dimension {key_value_states.size(-1)} doesn't match model dimension {self.d_model}\"\n",
    "\n",
    "\n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "        \n",
    "        # Linear projections and reshape for multi-head\n",
    "        Q_state = self.q_proj(sequence)\n",
    "        if is_cross_attention:\n",
    "            kv_seq_len = key_value_states.size(1)\n",
    "            K_state = self.k_proj(key_value_states)\n",
    "            V_state = self.v_proj(key_value_states)\n",
    "        else:\n",
    "            kv_seq_len = seq_len\n",
    "            K_state = self.k_proj(sequence)\n",
    "            V_state = self.v_proj(sequence)\n",
    "\n",
    "        #[batch_size, self.num_head, seq_len, self.d_head]\n",
    "        Q_state = Q_state.view(batch_size, seq_len, self.num_head, self.d_head).transpose(1,2) \n",
    "            \n",
    "        # in cross-attention, key/value sequence length might be different from query sequence length\n",
    "        K_state = K_state.view(batch_size, kv_seq_len, self.num_head, self.d_head).transpose(1,2)\n",
    "        V_state = V_state.view(batch_size, kv_seq_len, self.num_head, self.d_head).transpose(1,2)\n",
    "\n",
    "        # Scale Q by 1/sqrt(d_k)\n",
    "        Q_state = Q_state * self.scaler\n",
    "    \n",
    "    \n",
    "        # Compute attention matrix: QK^T\n",
    "        self.att_matrix = torch.matmul(Q_state, K_state.transpose(-1,-2)) \n",
    "\n",
    "    \n",
    "        # apply attention mask to attention matrix\n",
    "        if att_mask is not None and not isinstance(att_mask, torch.Tensor):\n",
    "            raise TypeError(\"att_mask must be a torch.Tensor\")\n",
    "\n",
    "        if att_mask is not None:\n",
    "            self.att_matrix = self.att_matrix + att_mask\n",
    "        \n",
    "        # apply softmax to the last dimension to get the attention score: softmax(QK^T)\n",
    "        att_score = F.softmax(self.att_matrix, dim = -1)\n",
    "    \n",
    "        # apply drop out to attention score\n",
    "        att_score = self.dropout(att_score)\n",
    "    \n",
    "        # get final output: softmax(QK^T)V\n",
    "        att_output = torch.matmul(att_score, V_state)\n",
    "    \n",
    "        # concatinate all attention heads\n",
    "        att_output = att_output.transpose(1, 2)\n",
    "        att_output = att_output.contiguous().view(batch_size, seq_len, self.num_head*self.d_head) \n",
    "    \n",
    "        # final linear transformation to the concatenated output\n",
    "        att_output = self.output_proj(att_output)\n",
    "\n",
    "        assert att_output.size() == (batch_size, seq_len, self.d_model), \\\n",
    "        f\"Final output shape {att_output.size()} incorrect\"\n",
    "\n",
    "        return att_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58d571-fab1-49e3-9d17-bb8e9ab8eae8",
   "metadata": {},
   "source": [
    "### Feed-Forward Network (FFN)\n",
    "Implements the position-wise feed-forward network from Section 3.3: FFN(x) = max(0, xW₁ + b₁)W₂ + b₂\n",
    "\n",
    "Key points:\n",
    "- Two linear transformations with ReLU in between\n",
    "- Inner layer dimension (d_ff) is typically 2048\n",
    "- Applied identically to each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bedf8d01-61e9-472b-836d-eb79d228da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Networks\n",
    "    This consists of two linear transformations with a ReLU activation in between.\n",
    "    \n",
    "    FFN(x) = max(0, xW1 + b1 )W2 + b2\n",
    "    d_model: embedding dimension (e.g., 512)\n",
    "    d_ff: feed-forward dimension (e.g., 2048)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.d_ff= d_ff\n",
    "        \n",
    "        # Linear transformation y = xW+b\n",
    "        self.fc1 = nn.Linear(self.d_model, self.d_ff, bias = True)\n",
    "        self.fc2 = nn.Linear(self.d_ff, self.d_model, bias = True)\n",
    "        \n",
    "        # for potential speed up\n",
    "        # Pre-normalize the weights (can help with training stability)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # check input and first FF layer dimension matching\n",
    "        batch_size, seq_length, d_input = input.size()\n",
    "        assert self.d_model == d_input, \"d_model must be the same dimension as the input\"\n",
    "\n",
    "        # First linear transformation followed by ReLU\n",
    "        # There's no need for explicit torch.max() as F.relu() already implements max(0,x)\n",
    "        f1 = F.relu(self.fc1(input))\n",
    "\n",
    "        # max(0, xW_1 + b_1)W_2 + b_2 \n",
    "        f2 =  self.fc2(f1)\n",
    "\n",
    "        return f2\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a555a9d-29a7-46a6-b163-06934db1aacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN(\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = FFN(  d_model = 512,  d_ff =2048)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c002f0-5562-4de2-97d3-aa65d596a3fb",
   "metadata": {},
   "source": [
    "### Transformer Encoder\n",
    "Implements single encoder layer from Section 3.1, consisting of:\n",
    "- Multi-head self-attention\n",
    "- Position-wise feed-forward network\n",
    "- Residual connections and layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89c2eebd-d774-4734-b9ea-486183182d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder layer of the Transformer\n",
    "    Sublayers: TransformerAttention\n",
    "               Residual LayerNorm\n",
    "               FNN\n",
    "               Residual LayerNorm\n",
    "    Args:\n",
    "            d_model: 512 model hidden dimension\n",
    "            d_embed: 512 embedding dimension, same as d_model in transformer framework\n",
    "            d_ff: 2048 hidden dimension of the feed forward network\n",
    "            num_head: 8 Number of attention heads.\n",
    "            dropout:  0.1 dropout rate \n",
    "            \n",
    "            bias: Whether to include bias in linear projections.\n",
    "              \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, d_model, d_ff,\n",
    "        num_head, dropout=0.1,\n",
    "        bias=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        \n",
    "        # attention sublayer\n",
    "        self.att = TransformerAttention(\n",
    "            d_model = d_model,\n",
    "            num_head = num_head,\n",
    "            dropout = dropout,\n",
    "            bias = bias\n",
    "        )\n",
    "        \n",
    "        # FFN sublayer\n",
    "        self.ffn = FFN(\n",
    "            d_model = d_model,\n",
    "            d_ff = d_ff\n",
    "        )\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # layer-normalization layer\n",
    "        self.LayerNorm_att = nn.LayerNorm(self.d_model)\n",
    "        self.LayerNorm_ffn = nn.LayerNorm(self.d_model)\n",
    "\n",
    "        \n",
    "    def forward(self, embed_input, padding_mask=None):\n",
    "       \n",
    "        batch_size, seq_len, _ = embed_input.size()\n",
    "        \n",
    "        ## First sublayer: self attion \n",
    "        att_sublayer = self.att(sequence = embed_input, key_value_states = None, \n",
    "                                att_mask = padding_mask)  # [batch_size, sequence_length, d_model]\n",
    "        \n",
    "        # apply dropout before layer normalization for each sublayer\n",
    "        att_sublayer = self.dropout(att_sublayer)\n",
    "        # Residual layer normalization\n",
    "        att_normalized = self.LayerNorm_att(embed_input + att_sublayer)           # [batch_size, sequence_length, d_model]\n",
    "        \n",
    "        ## Second sublayer: FFN\n",
    "        ffn_sublayer = self.ffn(att_normalized)                                   # [batch_size, sequence_length, d_model]\n",
    "        ffn_sublayer = self.dropout(ffn_sublayer)\n",
    "        ffn_normalized = self.LayerNorm_ffn(att_normalized + ffn_sublayer )       # [batch_size, sequence_length, d_model]\n",
    "    \n",
    "\n",
    "        return ffn_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bed1d90-9b3b-468d-a746-d1d0e1753c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEncoder(\n",
      "  (att): TransformerAttention(\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (output_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ffn): FFN(\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (LayerNorm_att): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (LayerNorm_ffn): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = TransformerEncoder( d_model = 512, d_ff =2048, num_head=8, dropout=0.1, bias=True )\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3233e1a-0107-4bfa-9507-d9c6c56cdd60",
   "metadata": {},
   "source": [
    "### Transformer Decoder\n",
    "Implements decoder layer from Section 3.1, with three sub-layers:\n",
    "- Masked multi-head self-attention\n",
    "- Multi-head cross-attention with encoder output\n",
    "- Position-wise feed-forward network\n",
    "\n",
    "Key points:\n",
    "- Self-attention uses causal masking\n",
    "- Cross-attention allows attending to all encoder outputs\n",
    "- Each sub-layer followed by residual connection and layer normalization\n",
    "- Apply dropout to the output of previous sub-layer before residual connection and layer normalization\n",
    "- Key implementation detail for causal masking:\n",
    "  - Create causal mask using upper triangular matrix:\n",
    "  ```python\n",
    "  mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "  mask = mask.masked_fill(mask == 1, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3aca0ac-3b48-4033-a889-dd019e2d67a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder layer of the Transformer\n",
    "    Sublayers: TransformerAttention with self-attention\n",
    "               Residual LayerNorm\n",
    "               TransformerAttention with cross-attention\n",
    "               Residual LayerNorm\n",
    "               FNN\n",
    "               Residual LayerNorm\n",
    "    Args:\n",
    "            d_model: 512 model hidden dimension\n",
    "            d_embed: 512 embedding dimension, same as d_model in transformer framework\n",
    "            d_ff: 2048 hidden dimension of the feed forward network\n",
    "            num_head: 8 Number of attention heads.\n",
    "            dropout:  0.1 dropout rate \n",
    "            \n",
    "            bias: Whether to include bias in linear projections.\n",
    "              \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, d_model, d_ff,\n",
    "        num_head, dropout=0.1,\n",
    "        bias=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        \n",
    "        # attention sublayer\n",
    "        self.att = TransformerAttention(\n",
    "            d_model = d_model,\n",
    "            num_head = num_head,\n",
    "            dropout = dropout,\n",
    "            bias = bias\n",
    "        )\n",
    "        \n",
    "        # FFN sublayer\n",
    "        self.ffn = FFN(\n",
    "            d_model = d_model,\n",
    "            d_ff = d_ff\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # layer-normalization layer\n",
    "        self.LayerNorm_att1 = nn.LayerNorm(self.d_model)\n",
    "        self.LayerNorm_att2 = nn.LayerNorm(self.d_model)\n",
    "        self.LayerNorm_ffn = nn.LayerNorm(self.d_model)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_causal_mask(seq_len):\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    \n",
    "    def forward(self, embed_input, cross_input, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        embed_input: Decoder input sequence [batch_size, seq_len, d_model]\n",
    "        cross_input: Encoder output sequence [batch_size, encoder_seq_len, d_model]\n",
    "        casual_attention_mask: Causal mask for self-attention [batch_size, seq_len, seq_len]\n",
    "        padding_mask: Padding mask for cross-attention [batch_size, seq_len, encoder_seq_len]\n",
    "        Returns:\n",
    "        Tensor: Decoded output [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = embed_input.size()\n",
    "        \n",
    "        assert embed_input.size(-1) == self.d_model, f\"Input dimension {embed_input.size(-1)} doesn't match model dimension {self.d_model}\"\n",
    "        assert cross_input.size(-1) == self.d_model, \"Encoder output dimension doesn't match model dimension\"\n",
    "\n",
    "\n",
    "        # Generate and expand causal mask for self-attention\n",
    "        causal_mask = self.create_causal_mask(seq_len).to(embed_input.device)  # [seq_len, seq_len]\n",
    "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(1)  # [1, 1, seq_len, seq_len]\n",
    "\n",
    "\n",
    "        ## First sublayer: self attion \n",
    "        # After embedding and positional encoding, input sequence feed into current attention sublayer\n",
    "        # Or, the output of the previous encoder/decoder feed into current attention sublayer\n",
    "        att_sublayer1 = self.att(sequence = embed_input, key_value_states = None, \n",
    "                                att_mask = causal_mask)  # [batch_size, num_head, sequence_length, d_model]\n",
    "        # apply dropout before layer normalization for each sublayer\n",
    "        att_sublayer1 = self.dropout(att_sublayer1)\n",
    "        # Residual layer normalization\n",
    "        att_normalized1 = self.LayerNorm_att1(embed_input + att_sublayer1)           # [batch_size, sequence_length, d_model]\n",
    "\n",
    "        ## Second sublayer: cross attention\n",
    "        # Query from the output of previous attention output, or training data\n",
    "        # Key, Value from output of Encoder of the same layer\n",
    "        att_sublayer2 = self.att(sequence = att_normalized1, key_value_states = cross_input, \n",
    "                                att_mask = padding_mask)  # [batch_size, sequence_length, d_model]\n",
    "        # apply dropout before layer normalization for each sublayer\n",
    "        att_sublayer2 = self.dropout(att_sublayer2)\n",
    "        # Residual layer normalization\n",
    "        att_normalized2 = self.LayerNorm_att2(att_normalized1 + att_sublayer2)           # [batch_size, sequence_length, d_model]\n",
    "        \n",
    "        \n",
    "        # Third sublayer: FFN\n",
    "        ffn_sublayer = self.ffn(att_normalized2)                                   # [batch_size, sequence_length, d_model]\n",
    "        ffn_sublayer = self.dropout(ffn_sublayer)\n",
    "        ffn_normalized = self.LayerNorm_ffn(att_normalized2 + ffn_sublayer )       # [batch_size, sequence_length, d_model]\n",
    "    \n",
    "\n",
    "        return ffn_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9678630-4e02-442f-9503-15066a574228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoder(\n",
      "  (att): TransformerAttention(\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (output_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ffn): FFN(\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (LayerNorm_att1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (LayerNorm_att2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (LayerNorm_ffn): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = TransformerDecoder( d_model = 512, d_ff =2048, num_head=8, dropout=0.1, bias=True )\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fe2c98-6278-4201-81f7-0b510efc7a68",
   "metadata": {},
   "source": [
    "### Encoder-Decoder Stack\n",
    "Implements the full stack of encoder and decoder layers from Section 3.1.\n",
    "\n",
    "Key points:\n",
    "- Multiple encoder and decoder layers (typically 6)\n",
    "- Each encoder output feeds into all decoder layers\n",
    "- Maintains residual connections throughout the stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aefccb90-9a89-4579-aba0-d837535f2d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder-Decoder stack of the Transformer\n",
    "    Sublayers:  Encoder x 6\n",
    "                Decoder x 6\n",
    "    Args:\n",
    "            d_model: 512 model hidden dimension\n",
    "            d_embed: 512 embedding dimension, same as d_model in transformer framework\n",
    "            d_ff: 2048 hidden dimension of the feed forward network\n",
    "            num_head: 8 Number of attention heads.\n",
    "            dropout:  0.1 dropout rate \n",
    "            \n",
    "            bias: Whether to include bias in linear projections.\n",
    "              \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, num_layer,\n",
    "        d_model, d_ff,\n",
    "        num_head, dropout=0.1,\n",
    "        bias=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.num_head = num_head\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        \n",
    "        # Encoder stack\n",
    "        self.encoder_stack = nn.ModuleList([ TransformerEncoder(\n",
    "                                        d_model = self.d_model, \n",
    "                                        d_ff = self.d_ff,\n",
    "                                        num_head = self.num_head, \n",
    "                                        dropout = self.dropout,\n",
    "                                        bias = self.bias) for _ in range(self.num_layer)])\n",
    "\n",
    "        # Decoder stack\n",
    "        self.decoder_stack = nn.ModuleList([ TransformerDecoder(\n",
    "                                        d_model = self.d_model, \n",
    "                                        d_ff = self.d_ff,\n",
    "                                        num_head = self.num_head, \n",
    "                                        dropout = self.dropout,\n",
    "                                        bias = self.bias) for _ in range(self.num_layer)])\n",
    "\n",
    "    \n",
    "    def forward(self, embed_encoder_input, embed_decoder_input, padding_mask=None):\n",
    "        # Process through all encoder layers first\n",
    "        encoder_output = embed_encoder_input\n",
    "        for encoder in self.encoder_stack:\n",
    "            encoder_output = encoder(encoder_output, padding_mask)\n",
    "        \n",
    "        # Use final encoder output for all decoder layers\n",
    "        decoder_output = embed_decoder_input\n",
    "        for decoder in self.decoder_stack:\n",
    "            decoder_output = decoder(decoder_output, encoder_output, padding_mask)\n",
    "        \n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacab71c-054e-41c1-b85a-23c886b0af61",
   "metadata": {},
   "source": [
    "### Full Transformer\n",
    "Combines all components into complete architecture:\n",
    "- Input embeddings for source and target\n",
    "- Positional encoding\n",
    "- Encoder-decoder stack\n",
    "- Final linear and softmax layer\n",
    "\n",
    "Key points:\n",
    "- Handles different vocabulary sizes for source/target\n",
    "- Shifts decoder inputs for teacher forcing\n",
    "- Projects outputs to target vocabulary size\n",
    "- Applies log softmax for training stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e86e2ade-4584-4905-89ec-beaac7ebf401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_layer,\n",
    "        d_model, d_embed, d_ff,\n",
    "        num_head,\n",
    "        src_vocab_size, \n",
    "        tgt_vocab_size,\n",
    "        max_position_embeddings=512,\n",
    "        dropout=0.1,\n",
    "        bias=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        \n",
    "        # Source and target embeddings\n",
    "        self.src_embedding = EmbeddingWithProjection(\n",
    "            vocab_size=src_vocab_size,\n",
    "            d_embed=d_embed,\n",
    "            d_model=d_model,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.tgt_embedding = EmbeddingWithProjection(\n",
    "            vocab_size=tgt_vocab_size,\n",
    "            d_embed=d_embed,\n",
    "            d_model=d_model,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Encoder-Decoder stack\n",
    "        self.encoder_decoder = TransformerEncoderDecoder(\n",
    "            num_layer=num_layer,\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            num_head=num_head,\n",
    "            dropout=dropout,\n",
    "            bias=bias\n",
    "        )\n",
    "        \n",
    "        # Output projection and softmax\n",
    "        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def shift_target_right(self, tgt_tokens):\n",
    "        # Shift target tokens right by padding with zeros at the beginning\n",
    "        batch_size, seq_len = tgt_tokens.size()\n",
    "        \n",
    "        # Create start token (zeros)\n",
    "        start_tokens = torch.zeros(batch_size, 1, dtype=tgt_tokens.dtype, device=tgt_tokens.device)\n",
    "        \n",
    "        # Concatenate start token and remove last token\n",
    "        shifted_tokens = torch.cat([start_tokens, tgt_tokens[:, :-1]], dim=1)\n",
    "        \n",
    "        return shifted_tokens\n",
    "        \n",
    "    def forward(self, src_tokens, tgt_tokens, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src_tokens: source sequence [batch_size, src_len]\n",
    "            tgt_tokens: target sequence [batch_size, tgt_len]\n",
    "            padding_mask: padding mask [batch_size, 1, 1, seq_len]\n",
    "        Returns:\n",
    "            output: [batch_size, tgt_len, tgt_vocab_size] log probabilities\n",
    "        \"\"\"\n",
    "        # Shift target tokens right for teacher forcing\n",
    "        shifted_tgt_tokens = self.shift_target_right(tgt_tokens)\n",
    "        \n",
    "        # Embed source and target sequences\n",
    "        src_embedding = self.src_embedding(src_tokens)\n",
    "        tgt_embedding = self.tgt_embedding(shifted_tgt_tokens)\n",
    "        \n",
    "        # Pass through encoder-decoder stack\n",
    "        decoder_output = self.encoder_decoder(\n",
    "            embed_encoder_input=src_embedding,\n",
    "            embed_decoder_input=tgt_embedding,\n",
    "            padding_mask=padding_mask\n",
    "        )\n",
    "        \n",
    "        # Project to vocabulary size and apply log softmax\n",
    "        logits = self.output_projection(decoder_output)\n",
    "        log_probs = self.softmax(logits)\n",
    "        \n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next: run the tests -> `transformer_testing.ipynb`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
